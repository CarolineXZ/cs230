{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5goiTDHK7ETD",
        "outputId": "3bbbf125-a821-46f0-b729-94223142db35"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/nanoGPT-master/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WNbibWYW7Gx-",
        "outputId": "e2d5791e-ee02-43e3-a447-2bf410f01d66"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/nanoGPT-master\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "IVQ_2Yd1BfVC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch numpy transformers datasets tiktoken wandb tqdm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D0UQPVav7IK6",
        "outputId": "912cbddd-b029-4a46-c0db-eaf646a20e5d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.26.4)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.46.2)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.1.0)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.10/dist-packages (0.8.0)\n",
            "Requirement already satisfied: wandb in /usr/local/lib/python3.10/dist-packages (0.18.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.6)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.9.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.26.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.10.10)\n",
            "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.7)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.1.43)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.10/dist-packages (from wandb) (4.3.6)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (4.25.5)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n",
            "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.18.0)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.10/dist-packages (from wandb) (1.3.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (75.1.0)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.17.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.11)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.1)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp->datasets) (0.2.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U --index-url https://aiinfra.pkgs.visualstudio.com/PublicPackages/_packaging/Triton-Nightly/pypi/simple/ triton-nightly"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AmeeJttV9zDf",
        "outputId": "bd26dec0-aa72-4254-d439-e159a64f3c70"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://aiinfra.pkgs.visualstudio.com/PublicPackages/_packaging/Triton-Nightly/pypi/simple/\n",
            "Collecting triton-nightly\n",
            "  Downloading https://aiinfra.pkgs.visualstudio.com/2692857e-05ef-43b4-ba9c-ccf1c22c437c/_packaging/07c94329-d4c3-4ad4-9e6b-f904a60032ec/pypi/download/triton-nightly/3.post20240716052845/triton_nightly-3.0.0.post20240716052845-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (138.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.2/138.2 MB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from triton-nightly) (3.16.1)\n",
            "Installing collected packages: triton-nightly\n",
            "Successfully installed triton-nightly-3.0.0.post20240716052845\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install fschat\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ty_eH09Pfb_",
        "outputId": "d862af16-1e82-42b3-a703-e64e6d8a19d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting fschat\n",
            "  Downloading fschat-0.2.36-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from fschat) (3.10.10)\n",
            "Collecting fastapi (from fschat)\n",
            "  Downloading fastapi-0.115.5-py3-none-any.whl.metadata (27 kB)\n",
            "Requirement already satisfied: httpx in /usr/local/lib/python3.10/dist-packages (from fschat) (0.27.2)\n",
            "Collecting markdown2[all] (from fschat)\n",
            "  Downloading markdown2-2.5.1-py2.py3-none-any.whl.metadata (2.2 kB)\n",
            "Collecting nh3 (from fschat)\n",
            "  Downloading nh3-0.2.18-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from fschat) (1.26.4)\n",
            "Requirement already satisfied: prompt-toolkit>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from fschat) (3.0.48)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.10/dist-packages (from fschat) (2.9.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from fschat) (2.32.3)\n",
            "Requirement already satisfied: rich>=10.0.0 in /usr/local/lib/python3.10/dist-packages (from fschat) (13.9.4)\n",
            "Collecting shortuuid (from fschat)\n",
            "  Downloading shortuuid-1.0.13-py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.10/dist-packages (from fschat) (0.8.0)\n",
            "Collecting uvicorn (from fschat)\n",
            "  Downloading uvicorn-0.32.0-py3-none-any.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit>=3.0.0->fschat) (0.2.13)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.0.0->fschat) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.0.0->fschat) (2.18.0)\n",
            "Requirement already satisfied: typing-extensions<5.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.0.0->fschat) (4.12.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->fschat) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->fschat) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->fschat) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->fschat) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->fschat) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->fschat) (1.17.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->fschat) (4.0.3)\n",
            "Collecting starlette<0.42.0,>=0.40.0 (from fastapi->fschat)\n",
            "  Downloading starlette-0.41.2-py3-none-any.whl.metadata (6.0 kB)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic->fschat) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic->fschat) (2.23.4)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx->fschat) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx->fschat) (2024.8.30)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx->fschat) (1.0.6)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx->fschat) (3.10)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx->fschat) (1.3.1)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx->fschat) (0.14.0)\n",
            "Collecting wavedrom (from markdown2[all]->fschat)\n",
            "  Downloading wavedrom-2.0.3.post3.tar.gz (137 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m137.7/137.7 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting latex2mathml (from markdown2[all]->fschat)\n",
            "  Downloading latex2mathml-3.77.0-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->fschat) (3.4.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->fschat) (2.2.3)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken->fschat) (2024.9.11)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn->fschat) (8.1.7)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.0.0->fschat) (0.1.2)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx->fschat) (1.2.2)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp->fschat) (0.2.0)\n",
            "Collecting svgwrite (from wavedrom->markdown2[all]->fschat)\n",
            "  Downloading svgwrite-1.4.3-py3-none-any.whl.metadata (8.8 kB)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from wavedrom->markdown2[all]->fschat) (1.16.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from wavedrom->markdown2[all]->fschat) (6.0.2)\n",
            "Downloading fschat-0.2.36-py3-none-any.whl (256 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m256.9/256.9 kB\u001b[0m \u001b[31m22.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fastapi-0.115.5-py3-none-any.whl (94 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.9/94.9 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nh3-0.2.18-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (769 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m769.2/769.2 kB\u001b[0m \u001b[31m17.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading shortuuid-1.0.13-py3-none-any.whl (10 kB)\n",
            "Downloading uvicorn-0.32.0-py3-none-any.whl (63 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.7/63.7 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading starlette-0.41.2-py3-none-any.whl (73 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.3/73.3 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading latex2mathml-3.77.0-py3-none-any.whl (73 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.7/73.7 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading markdown2-2.5.1-py2.py3-none-any.whl (48 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.4/48.4 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading svgwrite-1.4.3-py3-none-any.whl (67 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.1/67.1 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: wavedrom\n",
            "  Building wheel for wavedrom (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wavedrom: filename=wavedrom-2.0.3.post3-py2.py3-none-any.whl size=30052 sha256=e85aee7c1f727186e9fedf4e4c00174958c8f8fe86083d17bd8e1f03547879c3\n",
            "  Stored in directory: /root/.cache/pip/wheels/9c/52/8c/38b454b42f712f325e26f633287484c7dc1ad469e1580c5954\n",
            "Successfully built wavedrom\n",
            "Installing collected packages: nh3, uvicorn, svgwrite, shortuuid, markdown2, latex2mathml, wavedrom, starlette, fastapi, fschat\n",
            "Successfully installed fastapi-0.115.5 fschat-0.2.36 latex2mathml-3.77.0 markdown2-2.5.1 nh3-0.2.18 shortuuid-1.0.13 starlette-0.41.2 svgwrite-1.4.3 uvicorn-0.32.0 wavedrom-2.0.3.post3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python data/shakespeare/prepare.py\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9hkZvY6DOgLU",
        "outputId": "daabd880-56b2-4a61-91a8-8497adc982ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train has 301,966 tokens\n",
            "val has 36,059 tokens\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python train.py config/lora_shakespeare.py\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gXz50hjsGwY3",
        "outputId": "ead701fe-935f-4a4b-d277-e47917fc9aee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overriding config with config/lora_shakespeare.py:\n",
            "import time\n",
            "\n",
            "dataset = 'shakespeare'\n",
            "init_from = 'gpt2-medium'\n",
            "\n",
            "# LoRA parameters\n",
            "lora_rank = 32 * 4\n",
            "lora_alpha = 128 * 4\n",
            "lora_dropout = 0\n",
            "\n",
            "out_dir = f'out-lora-shakespeare-{init_from}-{lora_rank}-{lora_alpha}'\n",
            "eval_interval = 5\n",
            "eval_iters = 40\n",
            "wandb_log = False # feel free to turn on\n",
            "wandb_project = 'shakespeare'\n",
            "wandb_run_name = 'ft-' + str(time.time())\n",
            "\n",
            "# only save checkpoints if the validation loss improves\n",
            "always_save_checkpoint = False\n",
            "\n",
            "# the number of examples per iter:\n",
            "# 1 batch_size * 32 grad_accum * 1024 tokens = 32,768 tokens/iter\n",
            "# shakespeare has 301,966 tokens, so 1 epoch ~= 9.2 iters\n",
            "batch_size = 1\n",
            "gradient_accumulation_steps = 32\n",
            "max_iters = 20\n",
            "\n",
            "# finetune at constant LR\n",
            "learning_rate = 2e-4\n",
            "decay_lr = False\n",
            "\n",
            "device = 'cuda'\n",
            "compile = False\n",
            "compute_grad_memory = True\n",
            "\n",
            "\n",
            "total number of tokens per iteration: 262144\n",
            "Initializing from OpenAI GPT-2 weights: gpt2-medium\n",
            "2024-11-15 23:12:02.094317: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2024-11-15 23:12:02.118002: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-11-15 23:12:02.150541: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-11-15 23:12:02.161118: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-11-15 23:12:02.183759: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-11-15 23:12:03.572363: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "loading weights from pretrained gpt: gpt2-medium\n",
            "forcing vocab_size=50257, block_size=1024, bias=True\n",
            "overriding dropout rate to 0.0\n",
            "overriding lora_rank and lora_alpha to 128\n",
            "overriding lora_dropout to 0\n",
            "number of parameters: 372.65M\n",
            "Marking model as LoRA fine-tunable...\n",
            "Done.\n",
            "/content/drive/MyDrive/nanoGPT-master/train.py:221: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))\n",
            "step 0: train loss 3.5616, val loss 3.4638\n",
            "grad memory usage: 72.00 MB\n",
            "iter 0: loss 3.7337, time 21151.73ms, mfu -100.00%\n",
            "grad memory usage: 72.00 MB\n",
            "iter 1: loss 3.9769, time 18207.59ms, mfu -100.00%\n",
            "grad memory usage: 72.00 MB\n",
            "iter 2: loss 3.4331, time 17679.60ms, mfu -100.00%\n",
            "grad memory usage: 72.00 MB\n",
            "iter 3: loss 3.0857, time 18613.45ms, mfu -100.00%\n",
            "grad memory usage: 72.00 MB\n",
            "iter 4: loss 3.2436, time 18024.87ms, mfu -100.00%\n",
            "step 5: train loss 3.4297, val loss 3.1298\n",
            "saving checkpoint to out-lora-shakespeare-gpt2-medium-128-512\n",
            "grad memory usage: 72.00 MB\n",
            "iter 5: loss 3.0572, time 24368.89ms, mfu 8.75%\n",
            "grad memory usage: 72.00 MB\n",
            "iter 6: loss 3.2516, time 18468.62ms, mfu 9.03%\n",
            "grad memory usage: 72.00 MB\n",
            "iter 7: loss 3.4158, time 18151.08ms, mfu 9.30%\n",
            "grad memory usage: 72.00 MB\n",
            "iter 8: loss 3.3349, time 18300.62ms, mfu 9.54%\n",
            "grad memory usage: 72.00 MB\n",
            "iter 9: loss 3.6825, time 18001.54ms, mfu 9.77%\n",
            "step 10: train loss 3.3063, val loss 3.0994\n",
            "saving checkpoint to out-lora-shakespeare-gpt2-medium-128-512\n",
            "grad memory usage: 72.00 MB\n",
            "iter 10: loss 3.6806, time 24729.02ms, mfu 9.65%\n",
            "grad memory usage: 72.00 MB\n",
            "iter 11: loss 3.5165, time 18146.42ms, mfu 9.86%\n",
            "grad memory usage: 72.00 MB\n",
            "iter 12: loss 2.7808, time 17646.58ms, mfu 10.08%\n",
            "grad memory usage: 72.00 MB\n",
            "iter 13: loss 2.9538, time 18449.11ms, mfu 10.23%\n",
            "grad memory usage: 72.00 MB\n",
            "iter 14: loss 2.9295, time 18133.73ms, mfu 10.38%\n",
            "step 15: train loss 3.3604, val loss 3.0885\n",
            "saving checkpoint to out-lora-shakespeare-gpt2-medium-128-512\n",
            "grad memory usage: 72.00 MB\n",
            "iter 15: loss 3.5606, time 24827.40ms, mfu 10.21%\n",
            "grad memory usage: 72.00 MB\n",
            "iter 16: loss 3.8507, time 18233.01ms, mfu 10.35%\n",
            "grad memory usage: 72.00 MB\n",
            "iter 17: loss 3.3333, time 17633.10ms, mfu 10.53%\n",
            "grad memory usage: 72.00 MB\n",
            "iter 18: loss 3.1625, time 18068.13ms, mfu 10.66%\n",
            "grad memory usage: 72.00 MB\n",
            "iter 19: loss 2.8160, time 18038.51ms, mfu 10.77%\n",
            "step 20: train loss 3.2983, val loss 3.0685\n",
            "saving checkpoint to out-lora-shakespeare-gpt2-medium-128-512\n",
            "grad memory usage: 72.00 MB\n",
            "iter 20: loss 2.9984, time 24559.93ms, mfu 10.56%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python train.py config/finetune_shakespeare.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8b2IuZJ6SRvp",
        "outputId": "3037210f-3514-42cd-d8c2-d123eae33bf5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overriding config with config/finetune_shakespeare.py:\n",
            "import time\n",
            "\n",
            "dataset = 'shakespeare'\n",
            "init_from = 'gpt2'\n",
            "\n",
            "out_dir = f'out-shakespeare-{init_from}-2'\n",
            "eval_interval = 5\n",
            "eval_iters = 40\n",
            "wandb_log = False # feel free to turn on\n",
            "wandb_project = 'shakespeare'\n",
            "wandb_run_name = 'ft-' + str(time.time())\n",
            "\n",
            "# only save checkpoints if the validation loss improves\n",
            "always_save_checkpoint = False\n",
            "\n",
            "# the number of examples per iter:\n",
            "# 1 batch_size * 32 grad_accum * 1024 tokens = 32,768 tokens/iter\n",
            "# shakespeare has 301,966 tokens, so 1 epoch ~= 9.2 iters\n",
            "batch_size = 1\n",
            "gradient_accumulation_steps = 32\n",
            "max_iters = 20\n",
            "\n",
            "# finetune at constant LR\n",
            "learning_rate = 3e-5\n",
            "decay_lr = False\n",
            "\n",
            "# device = \"mps\"\n",
            "compile = False\n",
            "compute_grad_memory = True\n",
            "total number of tokens per iteration: 262144\n",
            "Initializing from OpenAI GPT-2 weights: gpt2\n",
            "loading weights from pretrained gpt: gpt2\n",
            "forcing vocab_size=50257, block_size=1024, bias=True\n",
            "overriding dropout rate to 0.0\n",
            "overriding lora_rank and lora_alpha to 0\n",
            "overriding lora_dropout to 0.0\n",
            "number of parameters: 123.65M\n",
            "/content/drive/MyDrive/nanoGPT-master/train.py:221: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))\n",
            "using fused AdamW: True\n",
            "step 0: train loss 4.1263, val loss 3.9790\n",
            "grad memory usage: 474.70 MB\n",
            "iter 0: loss 4.2106, time 9132.25ms, mfu -100.00%\n",
            "grad memory usage: 474.70 MB\n",
            "iter 1: loss 3.1506, time 7448.98ms, mfu -100.00%\n",
            "grad memory usage: 474.70 MB\n",
            "iter 2: loss 3.7413, time 7359.56ms, mfu -100.00%\n",
            "grad memory usage: 474.70 MB\n",
            "iter 3: loss 3.9072, time 7471.70ms, mfu -100.00%\n",
            "grad memory usage: 474.70 MB\n",
            "iter 4: loss 3.5413, time 7434.24ms, mfu -100.00%\n",
            "step 5: train loss 3.5743, val loss 3.4702\n",
            "saving checkpoint to out-shakespeare-gpt2-2\n",
            "grad memory usage: 474.70 MB\n",
            "iter 5: loss 3.9989, time 12292.35ms, mfu 5.85%\n",
            "grad memory usage: 474.70 MB\n",
            "iter 6: loss 3.0019, time 7970.03ms, mfu 6.16%\n",
            "grad memory usage: 474.70 MB\n",
            "iter 7: loss 3.9664, time 7896.69ms, mfu 6.46%\n",
            "grad memory usage: 474.70 MB\n",
            "iter 8: loss 3.6934, time 7690.73ms, mfu 6.74%\n",
            "grad memory usage: 474.70 MB\n",
            "iter 9: loss 3.6582, time 7342.92ms, mfu 7.05%\n",
            "step 10: train loss 3.5260, val loss 3.3916\n",
            "saving checkpoint to out-shakespeare-gpt2-2\n",
            "grad memory usage: 474.70 MB\n",
            "iter 10: loss 3.4822, time 12423.84ms, mfu 6.92%\n",
            "grad memory usage: 474.70 MB\n",
            "iter 11: loss 3.6380, time 8257.43ms, mfu 7.10%\n",
            "grad memory usage: 474.70 MB\n",
            "iter 12: loss 3.6590, time 7994.90ms, mfu 7.29%\n",
            "grad memory usage: 474.70 MB\n",
            "iter 13: loss 3.7222, time 7358.26ms, mfu 7.54%\n",
            "grad memory usage: 474.70 MB\n",
            "iter 14: loss 3.1595, time 7399.83ms, mfu 7.75%\n",
            "step 15: train loss 3.4822, val loss 3.3668\n",
            "saving checkpoint to out-shakespeare-gpt2-2\n",
            "grad memory usage: 474.70 MB\n",
            "iter 15: loss 3.5940, time 12151.47ms, mfu 7.57%\n",
            "grad memory usage: 474.70 MB\n",
            "iter 16: loss 3.3571, time 8238.94ms, mfu 7.68%\n",
            "grad memory usage: 474.70 MB\n",
            "iter 17: loss 3.4836, time 8055.73ms, mfu 7.81%\n",
            "grad memory usage: 474.70 MB\n",
            "iter 18: loss 2.8820, time 7425.86ms, mfu 7.99%\n",
            "grad memory usage: 474.70 MB\n",
            "iter 19: loss 3.6071, time 7391.30ms, mfu 8.17%\n",
            "step 20: train loss 3.4707, val loss 3.3805\n",
            "grad memory usage: 474.70 MB\n",
            "iter 20: loss 3.4822, time 8319.74ms, mfu 8.21%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python train.py config/lora_shakespeare.py\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YVoCxfgf49_D",
        "outputId": "af3c3f39-3afb-4403-9733-3d283e32f8d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overriding config with config/lora_shakespeare.py:\n",
            "import time\n",
            "\n",
            "dataset = 'shakespeare'\n",
            "init_from = 'gpt2'\n",
            "\n",
            "out_dir = f'out-lora-shakespeare-{init_from}-2'\n",
            "eval_interval = 5\n",
            "eval_iters = 40\n",
            "wandb_log = False # feel free to turn on\n",
            "wandb_project = 'shakespeare'\n",
            "wandb_run_name = 'ft-' + str(time.time())\n",
            "\n",
            "# only save checkpoints if the validation loss improves\n",
            "always_save_checkpoint = False\n",
            "\n",
            "# the number of examples per iter:\n",
            "# 1 batch_size * 32 grad_accum * 1024 tokens = 32,768 tokens/iter\n",
            "# shakespeare has 301,966 tokens, so 1 epoch ~= 9.2 iters\n",
            "batch_size = 1\n",
            "gradient_accumulation_steps = 32\n",
            "max_iters = 20\n",
            "\n",
            "# finetune at constant LR\n",
            "learning_rate = 2e-4\n",
            "decay_lr = False\n",
            "\n",
            "device = 'cuda'\n",
            "compile = False\n",
            "compute_grad_memory = True\n",
            "\n",
            "# LoRA parameters\n",
            "lora_rank = 32\n",
            "lora_alpha = 32\n",
            "lora_dropout = 0\n",
            "total number of tokens per iteration: 262144\n",
            "Initializing from OpenAI GPT-2 weights: gpt2\n",
            "2024-11-12 21:08:22.478755: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2024-11-12 21:08:22.495991: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-11-12 21:08:22.516717: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-11-12 21:08:22.523192: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-11-12 21:08:22.538486: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-11-12 21:08:23.724108: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "loading weights from pretrained gpt: gpt2\n",
            "forcing vocab_size=50257, block_size=1024, bias=True\n",
            "overriding dropout rate to 0.0\n",
            "overriding lora_rank and lora_alpha to 32\n",
            "overriding lora_dropout to 0\n",
            "number of parameters: 125.42M\n",
            "Marking model as LoRA fine-tunable...\n",
            "Done.\n",
            "/content/drive/MyDrive/nanoGPT-master/train.py:221: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))\n",
            "step 0: train loss 4.1790, val loss 4.0608\n",
            "grad memory usage: 6.75 MB\n",
            "iter 0: loss 4.6218, time 10762.32ms, mfu -100.00%\n",
            "grad memory usage: 6.75 MB\n",
            "iter 1: loss 3.8666, time 8600.79ms, mfu -100.00%\n",
            "grad memory usage: 6.75 MB\n",
            "iter 2: loss 3.9183, time 8560.20ms, mfu -100.00%\n",
            "grad memory usage: 6.75 MB\n",
            "iter 3: loss 3.7907, time 8584.36ms, mfu -100.00%\n",
            "grad memory usage: 6.75 MB\n",
            "iter 4: loss 4.2633, time 8608.30ms, mfu -100.00%\n",
            "step 5: train loss 3.8353, val loss 3.6377\n",
            "saving checkpoint to out-lora-shakespeare-gpt2-2\n",
            "grad memory usage: 6.75 MB\n",
            "iter 5: loss 3.7920, time 11237.83ms, mfu 6.47%\n",
            "grad memory usage: 6.75 MB\n",
            "iter 6: loss 3.4844, time 9158.50ms, mfu 6.62%\n",
            "grad memory usage: 6.75 MB\n",
            "iter 7: loss 3.7668, time 8647.85ms, mfu 6.80%\n",
            "grad memory usage: 6.75 MB\n",
            "iter 8: loss 2.8901, time 8626.19ms, mfu 6.96%\n",
            "grad memory usage: 6.75 MB\n",
            "iter 9: loss 3.1809, time 8576.98ms, mfu 7.11%\n",
            "step 10: train loss 3.7660, val loss 3.4913\n",
            "saving checkpoint to out-lora-shakespeare-gpt2-2\n",
            "grad memory usage: 6.75 MB\n",
            "iter 10: loss 3.2266, time 11037.10ms, mfu 7.06%\n",
            "grad memory usage: 6.75 MB\n",
            "iter 11: loss 3.5491, time 9635.62ms, mfu 7.11%\n",
            "grad memory usage: 6.75 MB\n",
            "iter 12: loss 3.3681, time 8718.09ms, mfu 7.23%\n",
            "grad memory usage: 6.75 MB\n",
            "iter 13: loss 3.7387, time 8642.88ms, mfu 7.35%\n",
            "grad memory usage: 6.75 MB\n",
            "iter 14: loss 3.9429, time 8556.82ms, mfu 7.47%\n",
            "step 15: train loss 3.7393, val loss 3.5684\n",
            "grad memory usage: 6.75 MB\n",
            "iter 15: loss 3.8654, time 9814.61ms, mfu 7.46%\n",
            "grad memory usage: 6.75 MB\n",
            "iter 16: loss 3.7297, time 8556.61ms, mfu 7.57%\n",
            "grad memory usage: 6.75 MB\n",
            "iter 17: loss 3.6470, time 8606.72ms, mfu 7.65%\n",
            "grad memory usage: 6.75 MB\n",
            "iter 18: loss 3.7203, time 8634.12ms, mfu 7.73%\n",
            "grad memory usage: 6.75 MB\n",
            "iter 19: loss 3.9108, time 8699.90ms, mfu 7.79%\n",
            "step 20: train loss 3.7186, val loss 3.4279\n",
            "saving checkpoint to out-lora-shakespeare-gpt2-2\n",
            "grad memory usage: 6.75 MB\n",
            "iter 20: loss 4.1052, time 11202.48ms, mfu 7.66%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python data/openwebtext/prepare.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BQIZTVuhOh-y",
        "outputId": "48946ba2-7435-46de-9c6f-70b448ae2b55"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "README.md: 100% 7.35k/7.35k [00:00<00:00, 35.5MB/s]\n",
            "openwebtext.py: 100% 2.73k/2.73k [00:00<00:00, 20.4MB/s]\n",
            "The repository for openwebtext contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/openwebtext.\n",
            "You can avoid this prompt in future by passing the argument `trust_remote_code=True`.\n",
            "\n",
            "Do you wish to run the custom code? [y/N] y\n",
            "urlsf_subset06.tar: 100% 626M/626M [00:08<00:00, 71.7MB/s]\n",
            "urlsf_subset00.tar: 100% 633M/633M [00:08<00:00, 72.5MB/s]\n",
            "urlsf_subset03.tar: 100% 628M/628M [00:08<00:00, 72.6MB/s]\n",
            "urlsf_subset15.tar: 100% 621M/621M [00:08<00:00, 69.6MB/s]\n",
            "urlsf_subset09.tar: 100% 626M/626M [00:08<00:00, 70.1MB/s]\n",
            "urlsf_subset18.tar: 100% 618M/618M [00:09<00:00, 67.4MB/s]\n",
            "urlsf_subset12.tar: 100% 624M/624M [00:09<00:00, 66.2MB/s]\n",
            "urlsf_subset04.tar: 100% 627M/627M [00:10<00:00, 61.7MB/s]\n",
            "urlsf_subset07.tar: 100% 625M/625M [00:12<00:00, 48.4MB/s]\n",
            "urlsf_subset01.tar: 100% 629M/629M [00:13<00:00, 48.3MB/s]\n",
            "urlsf_subset16.tar: 100% 619M/619M [00:13<00:00, 46.4MB/s]\n",
            "urlsf_subset10.tar: 100% 625M/625M [00:14<00:00, 42.7MB/s]\n",
            "urlsf_subset13.tar: 100% 629M/629M [00:14<00:00, 43.7MB/s]\n",
            "urlsf_subset19.tar: 100% 619M/619M [00:15<00:00, 38.8MB/s]\n",
            "urlsf_subset05.tar: 100% 630M/630M [00:13<00:00, 47.4MB/s]\n",
            "urlsf_subset20.tar: 100% 377M/377M [00:07<00:00, 53.7MB/s]\n",
            "urlsf_subset08.tar: 100% 625M/625M [00:11<00:00, 53.9MB/s]\n",
            "urlsf_subset02.tar: 100% 629M/629M [00:11<00:00, 54.2MB/s]\n",
            "urlsf_subset17.tar: 100% 619M/619M [00:11<00:00, 54.9MB/s]\n",
            "urlsf_subset11.tar: 100% 625M/625M [00:10<00:00, 62.5MB/s]\n",
            "urlsf_subset14.tar: 100% 627M/627M [00:10<00:00, 58.6MB/s]\n",
            "Generating train split:  25% 1974460/8013769 [01:15<03:39, 27451.58 examples/s]Process ForkPoolWorker-8:\n",
            "Process ForkPoolWorker-4:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/multiprocess/process.py\", line 314, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/multiprocess/process.py\", line 108, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/multiprocess/pool.py\", line 125, in worker\n",
            "    result = (True, func(*args, **kwds))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/datasets/utils/py_utils.py\", line 678, in _write_generator_to_queue\n",
            "    for i, result in enumerate(func(**kwargs)):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/datasets/builder.py\", line 1607, in _prepare_split_single\n",
            "    for key, record in generator:\n",
            "  File \"/root/.cache/huggingface/modules/datasets_modules/datasets/openwebtext/6f68e85c16ccc770c0dd489f4008852ea9633604995addd0cd76e293aed9e521/openwebtext.py\", line 79, in _generate_examples\n",
            "    yield idx, {\"text\": re.sub(\"\\n\\n\\n+\", \"\\n\\n\", txt_f.read().decode(\"utf-8\")).strip()}\n",
            "  File \"/usr/lib/python3.10/tarfile.py\", line 689, in read\n",
            "    b = self.fileobj.read(length)\n",
            "  File \"/usr/lib/python3.10/tarfile.py\", line 526, in read\n",
            "    buf = self._read(size)\n",
            "  File \"/usr/lib/python3.10/tarfile.py\", line 548, in _read\n",
            "    buf = self.cmp.decompress(buf)\n",
            "Traceback (most recent call last):\n",
            "KeyboardInterrupt\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/multiprocess/process.py\", line 314, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/multiprocess/process.py\", line 108, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/multiprocess/pool.py\", line 125, in worker\n",
            "    result = (True, func(*args, **kwds))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/datasets/utils/py_utils.py\", line 678, in _write_generator_to_queue\n",
            "    for i, result in enumerate(func(**kwargs)):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/datasets/builder.py\", line 1607, in _prepare_split_single\n",
            "    for key, record in generator:\n",
            "  File \"/root/.cache/huggingface/modules/datasets_modules/datasets/openwebtext/6f68e85c16ccc770c0dd489f4008852ea9633604995addd0cd76e293aed9e521/openwebtext.py\", line 79, in _generate_examples\n",
            "    yield idx, {\"text\": re.sub(\"\\n\\n\\n+\", \"\\n\\n\", txt_f.read().decode(\"utf-8\")).strip()}\n",
            "  File \"/usr/lib/python3.10/tarfile.py\", line 689, in read\n",
            "    b = self.fileobj.read(length)\n",
            "  File \"/usr/lib/python3.10/tarfile.py\", line 526, in read\n",
            "    buf = self._read(size)\n",
            "  File \"/usr/lib/python3.10/tarfile.py\", line 548, in _read\n",
            "    buf = self.cmp.decompress(buf)\n",
            "KeyboardInterrupt\n",
            "Process ForkPoolWorker-3:\n",
            "Process ForkPoolWorker-2:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/multiprocess/process.py\", line 314, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/multiprocess/process.py\", line 108, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/multiprocess/pool.py\", line 125, in worker\n",
            "    result = (True, func(*args, **kwds))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/datasets/utils/py_utils.py\", line 678, in _write_generator_to_queue\n",
            "    for i, result in enumerate(func(**kwargs)):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/datasets/builder.py\", line 1607, in _prepare_split_single\n",
            "    for key, record in generator:\n",
            "  File \"/root/.cache/huggingface/modules/datasets_modules/datasets/openwebtext/6f68e85c16ccc770c0dd489f4008852ea9633604995addd0cd76e293aed9e521/openwebtext.py\", line 79, in _generate_examples\n",
            "    yield idx, {\"text\": re.sub(\"\\n\\n\\n+\", \"\\n\\n\", txt_f.read().decode(\"utf-8\")).strip()}\n",
            "  File \"/usr/lib/python3.10/tarfile.py\", line 689, in read\n",
            "    b = self.fileobj.read(length)\n",
            "  File \"/usr/lib/python3.10/tarfile.py\", line 526, in read\n",
            "    buf = self._read(size)\n",
            "  File \"/usr/lib/python3.10/tarfile.py\", line 548, in _read\n",
            "    buf = self.cmp.decompress(buf)\n",
            "KeyboardInterrupt\n",
            "Process ForkPoolWorker-6:\n",
            "Generating train split:  25% 1976420/8013769 [01:15<03:51, 26050.33 examples/s]\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/multiprocess/process.py\", line 314, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/multiprocess/process.py\", line 108, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/multiprocess/pool.py\", line 125, in worker\n",
            "    result = (True, func(*args, **kwds))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/datasets/utils/py_utils.py\", line 678, in _write_generator_to_queue\n",
            "    for i, result in enumerate(func(**kwargs)):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/datasets/builder.py\", line 1607, in _prepare_split_single\n",
            "    for key, record in generator:\n",
            "  File \"/root/.cache/huggingface/modules/datasets_modules/datasets/openwebtext/6f68e85c16ccc770c0dd489f4008852ea9633604995addd0cd76e293aed9e521/openwebtext.py\", line 79, in _generate_examples\n",
            "    yield idx, {\"text\": re.sub(\"\\n\\n\\n+\", \"\\n\\n\", txt_f.read().decode(\"utf-8\")).strip()}\n",
            "  File \"/usr/lib/python3.10/tarfile.py\", line 689, in read\n",
            "    b = self.fileobj.read(length)\n",
            "  File \"/usr/lib/python3.10/tarfile.py\", line 526, in read\n",
            "    buf = self._read(size)\n",
            "  File \"/usr/lib/python3.10/tarfile.py\", line 548, in _read\n",
            "    buf = self.cmp.decompress(buf)\n",
            "KeyboardInterrupt\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/datasets/utils/py_utils.py\", line 704, in iflatmap_unordered\n",
            "    yield queue.get(timeout=0.05)\n",
            "  File \"<string>\", line 2, in get\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/multiprocess/managers.py\", line 818, in _callmethod\n",
            "    kind, result = conn.recv()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/multiprocess/connection.py\", line 253, in recv\n",
            "    buf = self._recv_bytes()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/multiprocess/connection.py\", line 417, in _recv_bytes\n",
            "    buf = self._recv(4)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/multiprocess/connection.py\", line 382, in _recv\n",
            "    chunk = read(handle, remaining)\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/drive/MyDrive/nanoGPT-master/data/openwebtext/prepare.py\", line 23, in <module>\n",
            "    dataset = load_dataset(\"openwebtext\", num_proc=num_proc_load_dataset)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/datasets/load.py\", line 2154, in load_dataset\n",
            "    builder_instance.download_and_prepare(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/datasets/builder.py\", line 924, in download_and_prepare\n",
            "    self._download_and_prepare(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/datasets/builder.py\", line 1648, in _download_and_prepare\n",
            "    super()._download_and_prepare(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/datasets/builder.py\", line 1000, in _download_and_prepare\n",
            "    self._prepare_split(split_generator, **prepare_split_kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/datasets/builder.py\", line 1515, in _prepare_split\n",
            "    for job_id, done, content in iflatmap_unordered(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/datasets/utils/py_utils.py\", line 718, in iflatmap_unordered\n",
            "    [async_result.get(timeout=0.05) for async_result in async_results]\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/datasets/utils/py_utils.py\", line 718, in <listcomp>\n",
            "    [async_result.get(timeout=0.05) for async_result in async_results]\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/multiprocess/pool.py\", line 768, in get\n",
            "    self.wait(timeout)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/multiprocess/pool.py\", line 765, in wait\n",
            "    self._event.wait(timeout)\n",
            "  File \"/usr/lib/python3.10/threading.py\", line 607, in wait\n",
            "    signaled = self._cond.wait(timeout)\n",
            "  File \"/usr/lib/python3.10/threading.py\", line 324, in wait\n",
            "    gotit = waiter.acquire(True, timeout)\n",
            "KeyboardInterrupt\n",
            "^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python train.py config/finetune_shakespeare.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D36eDRq7PbKC",
        "outputId": "0c33d1cb-46c9-4e09-8d17-c839e035c0dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overriding config with config/finetune_shakespeare.py:\n",
            "import time\n",
            "\n",
            "dataset = 'shakespeare'\n",
            "init_from = 'gpt2-medium'\n",
            "\n",
            "out_dir = f'out-shakespeare-{init_from}'\n",
            "eval_interval = 5\n",
            "eval_iters = 40\n",
            "wandb_log = False # feel free to turn on\n",
            "wandb_project = 'shakespeare'\n",
            "wandb_run_name = 'ft-' + str(time.time())\n",
            "\n",
            "# only save checkpoints if the validation loss improves\n",
            "always_save_checkpoint = False\n",
            "\n",
            "# the number of examples per iter:\n",
            "# 1 batch_size * 32 grad_accum * 1024 tokens = 32,768 tokens/iter\n",
            "# shakespeare has 301,966 tokens, so 1 epoch ~= 9.2 iters\n",
            "batch_size = 1\n",
            "gradient_accumulation_steps = 32\n",
            "max_iters = 20\n",
            "\n",
            "# finetune at constant LR\n",
            "learning_rate = 3e-5\n",
            "decay_lr = False\n",
            "\n",
            "# device = \"mps\"\n",
            "compile = False\n",
            "compute_grad_memory = True\n",
            "total number of tokens per iteration: 262144\n",
            "Initializing from OpenAI GPT-2 weights: gpt2-medium\n",
            "2024-11-12 21:18:15.873193: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2024-11-12 21:18:15.890290: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-11-12 21:18:15.910972: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-11-12 21:18:15.917385: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-11-12 21:18:15.932403: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-11-12 21:18:17.088453: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "loading weights from pretrained gpt: gpt2-medium\n",
            "forcing vocab_size=50257, block_size=1024, bias=True\n",
            "overriding dropout rate to 0.0\n",
            "overriding lora_rank and lora_alpha to 0\n",
            "overriding lora_dropout to 0.0\n",
            "number of parameters: 353.77M\n",
            "config.json: 100% 718/718 [00:00<00:00, 5.25MB/s]\n",
            "model.safetensors: 100% 1.52G/1.52G [00:06<00:00, 231MB/s]\n",
            "generation_config.json: 100% 124/124 [00:00<00:00, 1.01MB/s]\n",
            "/content/drive/MyDrive/nanoGPT-master/train.py:221: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))\n",
            "using fused AdamW: True\n",
            "step 0: train loss 3.5964, val loss 3.4175\n",
            "grad memory usage: 1353.54 MB\n",
            "iter 0: loss 3.4379, time 16188.19ms, mfu -100.00%\n",
            "grad memory usage: 1353.54 MB\n",
            "iter 1: loss 3.2313, time 13778.36ms, mfu -100.00%\n",
            "grad memory usage: 1353.54 MB\n",
            "iter 2: loss 2.5297, time 13713.96ms, mfu -100.00%\n",
            "grad memory usage: 1353.54 MB\n",
            "iter 3: loss 3.6009, time 13750.03ms, mfu -100.00%\n",
            "grad memory usage: 1353.54 MB\n",
            "iter 4: loss 3.3420, time 13547.23ms, mfu -100.00%\n",
            "step 5: train loss 3.3367, val loss 3.1555\n",
            "saving checkpoint to out-shakespeare-gpt2-medium\n",
            "grad memory usage: 1353.54 MB\n",
            "iter 5: loss 3.6537, time 27298.89ms, mfu 7.46%\n",
            "grad memory usage: 1353.54 MB\n",
            "iter 6: loss 3.1497, time 13885.55ms, mfu 8.18%\n",
            "grad memory usage: 1353.54 MB\n",
            "iter 7: loss 3.3517, time 14884.84ms, mfu 8.73%\n",
            "grad memory usage: 1353.54 MB\n",
            "iter 8: loss 3.9021, time 14659.35ms, mfu 9.25%\n",
            "grad memory usage: 1353.54 MB\n",
            "iter 9: loss 3.0382, time 13705.96ms, mfu 9.81%\n",
            "step 10: train loss 3.2638, val loss 3.1582\n",
            "grad memory usage: 1353.54 MB\n",
            "iter 10: loss 3.5875, time 15386.21ms, mfu 10.15%\n",
            "grad memory usage: 1353.54 MB\n",
            "iter 11: loss 3.1951, time 13702.61ms, mfu 10.63%\n",
            "grad memory usage: 1353.54 MB\n",
            "iter 12: loss 3.1603, time 13577.62ms, mfu 11.06%\n",
            "grad memory usage: 1353.54 MB\n",
            "iter 13: loss 3.2415, time 13635.82ms, mfu 11.45%\n",
            "grad memory usage: 1353.54 MB\n",
            "iter 14: loss 3.2619, time 13691.84ms, mfu 11.79%\n",
            "step 15: train loss 3.2056, val loss 3.0972\n",
            "saving checkpoint to out-shakespeare-gpt2-medium\n",
            "grad memory usage: 1353.54 MB\n",
            "iter 15: loss 3.6632, time 27525.73ms, mfu 11.35%\n",
            "grad memory usage: 1353.54 MB\n",
            "iter 16: loss 3.1966, time 17117.39ms, mfu 11.41%\n",
            "grad memory usage: 1353.54 MB\n",
            "iter 17: loss 2.9875, time 15931.84ms, mfu 11.55%\n",
            "grad memory usage: 1353.54 MB\n",
            "iter 18: loss 3.5738, time 13930.72ms, mfu 11.85%\n",
            "grad memory usage: 1353.54 MB\n",
            "iter 19: loss 3.6408, time 13925.58ms, mfu 12.13%\n",
            "step 20: train loss 3.1042, val loss 3.0306\n",
            "saving checkpoint to out-shakespeare-gpt2-medium\n",
            "grad memory usage: 1353.54 MB\n",
            "iter 20: loss 3.2689, time 29606.77ms, mfu 11.61%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python train.py config/lora_shakespeare.py\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W1-e07lcPbZU",
        "outputId": "49814977-435b-4539-f38e-ce74de8d3c2f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overriding config with config/lora_shakespeare.py:\n",
            "import time\n",
            "\n",
            "dataset = 'shakespeare'\n",
            "init_from = 'gpt2-medium'\n",
            "\n",
            "out_dir = f'out-lora-shakespeare-{init_from}'\n",
            "eval_interval = 5\n",
            "eval_iters = 40\n",
            "wandb_log = False # feel free to turn on\n",
            "wandb_project = 'shakespeare'\n",
            "wandb_run_name = 'ft-' + str(time.time())\n",
            "\n",
            "# only save checkpoints if the validation loss improves\n",
            "always_save_checkpoint = False\n",
            "\n",
            "# the number of examples per iter:\n",
            "# 1 batch_size * 32 grad_accum * 1024 tokens = 32,768 tokens/iter\n",
            "# shakespeare has 301,966 tokens, so 1 epoch ~= 9.2 iters\n",
            "batch_size = 1\n",
            "gradient_accumulation_steps = 32\n",
            "max_iters = 20\n",
            "\n",
            "# finetune at constant LR\n",
            "learning_rate = 2e-4\n",
            "decay_lr = False\n",
            "\n",
            "device = 'cuda'\n",
            "compile = False\n",
            "compute_grad_memory = True\n",
            "\n",
            "# LoRA parameters\n",
            "lora_rank = 8\n",
            "lora_alpha = 32\n",
            "lora_dropout = 0\n",
            "total number of tokens per iteration: 262144\n",
            "Initializing from OpenAI GPT-2 weights: gpt2-medium\n",
            "2024-11-12 21:26:20.763419: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2024-11-12 21:26:20.784794: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-11-12 21:26:20.805842: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-11-12 21:26:20.812327: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-11-12 21:26:20.827946: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-11-12 21:26:22.016859: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "loading weights from pretrained gpt: gpt2-medium\n",
            "forcing vocab_size=50257, block_size=1024, bias=True\n",
            "overriding dropout rate to 0.0\n",
            "overriding lora_rank and lora_alpha to 8\n",
            "overriding lora_dropout to 0\n",
            "number of parameters: 354.95M\n",
            "Marking model as LoRA fine-tunable...\n",
            "Done.\n",
            "/content/drive/MyDrive/nanoGPT-master/train.py:221: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))\n",
            "step 0: train loss 3.6547, val loss 3.3857\n",
            "grad memory usage: 4.50 MB\n",
            "iter 0: loss 3.6511, time 20127.91ms, mfu -100.00%\n",
            "grad memory usage: 4.50 MB\n",
            "iter 1: loss 3.4206, time 17280.09ms, mfu -100.00%\n",
            "grad memory usage: 4.50 MB\n",
            "iter 2: loss 3.8277, time 17163.86ms, mfu -100.00%\n",
            "grad memory usage: 4.50 MB\n",
            "iter 3: loss 3.7967, time 17061.81ms, mfu -100.00%\n",
            "grad memory usage: 4.50 MB\n",
            "iter 4: loss 3.2993, time 17214.86ms, mfu -100.00%\n",
            "step 5: train loss 3.4909, val loss 3.3434\n",
            "saving checkpoint to out-lora-shakespeare-gpt2-medium\n",
            "grad memory usage: 4.50 MB\n",
            "iter 5: loss 3.2654, time 26996.67ms, mfu 7.57%\n",
            "grad memory usage: 4.50 MB\n",
            "iter 6: loss 3.7185, time 17087.50ms, mfu 8.01%\n",
            "grad memory usage: 4.50 MB\n",
            "iter 7: loss 3.6519, time 17183.52ms, mfu 8.40%\n",
            "grad memory usage: 4.50 MB\n",
            "iter 8: loss 3.6357, time 17090.07ms, mfu 8.75%\n",
            "grad memory usage: 4.50 MB\n",
            "iter 9: loss 3.4830, time 17096.27ms, mfu 9.07%\n",
            "step 10: train loss 3.4193, val loss 3.2907\n",
            "saving checkpoint to out-lora-shakespeare-gpt2-medium\n",
            "grad memory usage: 4.50 MB\n",
            "iter 10: loss 3.0886, time 24452.68ms, mfu 9.00%\n",
            "grad memory usage: 4.50 MB\n",
            "iter 11: loss 3.6212, time 17277.00ms, mfu 9.28%\n",
            "grad memory usage: 4.50 MB\n",
            "iter 12: loss 3.9170, time 17051.57ms, mfu 9.55%\n",
            "grad memory usage: 4.50 MB\n",
            "iter 13: loss 3.1827, time 16997.52ms, mfu 9.80%\n",
            "grad memory usage: 4.50 MB\n",
            "iter 14: loss 3.4569, time 17113.51ms, mfu 10.01%\n",
            "step 15: train loss 3.4160, val loss 3.3068\n",
            "grad memory usage: 4.50 MB\n",
            "iter 15: loss 3.1943, time 19421.39ms, mfu 10.06%\n",
            "grad memory usage: 4.50 MB\n",
            "iter 16: loss 3.1541, time 17088.93ms, mfu 10.25%\n",
            "grad memory usage: 4.50 MB\n",
            "iter 17: loss 3.2947, time 17019.12ms, mfu 10.43%\n",
            "grad memory usage: 4.50 MB\n",
            "iter 18: loss 3.0607, time 17113.11ms, mfu 10.58%\n",
            "grad memory usage: 4.50 MB\n",
            "iter 19: loss 3.1735, time 17016.84ms, mfu 10.72%\n",
            "step 20: train loss 3.3965, val loss 3.2381\n",
            "saving checkpoint to out-lora-shakespeare-gpt2-medium\n",
            "grad memory usage: 4.50 MB\n",
            "iter 20: loss 3.4398, time 24403.57ms, mfu 10.49%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python train.py config/lora_shakespeare.py\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v8cvsWCgPWWR",
        "outputId": "fe7e488d-5d85-47b3-b203-c2f8db15a7d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overriding config with config/lora_shakespeare.py:\n",
            "import time\n",
            "\n",
            "dataset = 'shakespeare'\n",
            "init_from = 'gpt2-medium'\n",
            "\n",
            "out_dir = f'out-lora-shakespeare-{init_from}'\n",
            "eval_interval = 5\n",
            "eval_iters = 40\n",
            "wandb_log = False # feel free to turn on\n",
            "wandb_project = 'shakespeare'\n",
            "wandb_run_name = 'ft-' + str(time.time())\n",
            "\n",
            "# only save checkpoints if the validation loss improves\n",
            "always_save_checkpoint = False\n",
            "\n",
            "# the number of examples per iter:\n",
            "# 1 batch_size * 32 grad_accum * 1024 tokens = 32,768 tokens/iter\n",
            "# shakespeare has 301,966 tokens, so 1 epoch ~= 9.2 iters\n",
            "batch_size = 1\n",
            "gradient_accumulation_steps = 32\n",
            "max_iters = 20\n",
            "\n",
            "# finetune at constant LR\n",
            "learning_rate = 2e-4\n",
            "decay_lr = False\n",
            "\n",
            "device = 'cuda'\n",
            "compile = False\n",
            "compute_grad_memory = True\n",
            "\n",
            "# LoRA parameters\n",
            "lora_rank = 16\n",
            "lora_alpha = 64\n",
            "lora_dropout = 0\n",
            "total number of tokens per iteration: 262144\n",
            "Initializing from OpenAI GPT-2 weights: gpt2-medium\n",
            "2024-11-12 21:42:38.257209: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2024-11-12 21:42:38.274839: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-11-12 21:42:38.296006: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-11-12 21:42:38.302470: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-11-12 21:42:38.317901: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-11-12 21:42:39.492891: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "loading weights from pretrained gpt: gpt2-medium\n",
            "forcing vocab_size=50257, block_size=1024, bias=True\n",
            "overriding dropout rate to 0.0\n",
            "overriding lora_rank and lora_alpha to 16\n",
            "overriding lora_dropout to 0\n",
            "number of parameters: 356.13M\n",
            "Marking model as LoRA fine-tunable...\n",
            "Done.\n",
            "/content/drive/MyDrive/nanoGPT-master/train.py:221: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))\n",
            "step 0: train loss 3.5893, val loss 3.4092\n",
            "grad memory usage: 9.00 MB\n",
            "iter 0: loss 3.6829, time 20448.93ms, mfu -100.00%\n",
            "grad memory usage: 9.00 MB\n",
            "iter 1: loss 3.6488, time 17628.92ms, mfu -100.00%\n",
            "grad memory usage: 9.00 MB\n",
            "iter 2: loss 3.1281, time 17613.83ms, mfu -100.00%\n",
            "grad memory usage: 9.00 MB\n",
            "iter 3: loss 3.5276, time 17379.31ms, mfu -100.00%\n",
            "grad memory usage: 9.00 MB\n",
            "iter 4: loss 3.3573, time 17466.99ms, mfu -100.00%\n",
            "step 5: train loss 3.4745, val loss 3.3024\n",
            "saving checkpoint to out-lora-shakespeare-gpt2-medium\n",
            "grad memory usage: 9.00 MB\n",
            "iter 5: loss 3.5490, time 23614.72ms, mfu 8.68%\n",
            "grad memory usage: 9.00 MB\n",
            "iter 6: loss 3.7674, time 17684.79ms, mfu 8.97%\n",
            "grad memory usage: 9.00 MB\n",
            "iter 7: loss 3.2520, time 17315.76ms, mfu 9.25%\n",
            "grad memory usage: 9.00 MB\n",
            "iter 8: loss 2.7195, time 17447.75ms, mfu 9.50%\n",
            "grad memory usage: 9.00 MB\n",
            "iter 9: loss 3.6809, time 17319.68ms, mfu 9.74%\n",
            "step 10: train loss 3.3841, val loss 3.2099\n",
            "saving checkpoint to out-lora-shakespeare-gpt2-medium\n",
            "grad memory usage: 9.00 MB\n",
            "iter 10: loss 3.6229, time 24728.84ms, mfu 9.59%\n",
            "grad memory usage: 9.00 MB\n",
            "iter 11: loss 3.5081, time 17559.54ms, mfu 9.80%\n",
            "grad memory usage: 9.00 MB\n",
            "iter 12: loss 3.2842, time 17483.36ms, mfu 9.99%\n",
            "grad memory usage: 9.00 MB\n",
            "iter 13: loss 3.7327, time 17483.24ms, mfu 10.16%\n",
            "grad memory usage: 9.00 MB\n",
            "iter 14: loss 3.1807, time 17643.06ms, mfu 10.31%\n",
            "step 15: train loss 3.2722, val loss 3.2369\n",
            "grad memory usage: 9.00 MB\n",
            "iter 15: loss 3.6495, time 19363.39ms, mfu 10.34%\n",
            "grad memory usage: 9.00 MB\n",
            "iter 16: loss 3.1436, time 17474.91ms, mfu 10.48%\n",
            "grad memory usage: 9.00 MB\n",
            "iter 17: loss 3.7376, time 17294.76ms, mfu 10.61%\n",
            "grad memory usage: 9.00 MB\n",
            "iter 18: loss 3.7452, time 17420.66ms, mfu 10.73%\n",
            "grad memory usage: 9.00 MB\n",
            "iter 19: loss 3.1668, time 17237.55ms, mfu 10.84%\n",
            "step 20: train loss 3.4141, val loss 3.2389\n",
            "grad memory usage: 9.00 MB\n",
            "iter 20: loss 3.7702, time 19661.38ms, mfu 10.80%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python train.py config/lora_shakespeare.py\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hG7TjkRnX1An",
        "outputId": "e1e00ab2-d78c-4802-c1b3-0d571341d17e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overriding config with config/lora_shakespeare.py:\n",
            "import time\n",
            "\n",
            "dataset = 'shakespeare'\n",
            "init_from = 'gpt2'\n",
            "\n",
            "# LoRA parameters\n",
            "lora_rank = 16\n",
            "lora_alpha = 64\n",
            "lora_dropout = 0\n",
            "\n",
            "out_dir = f'out-lora-shakespeare-{init_from}-{lora_rank}-{lora_alpha}'\n",
            "eval_interval = 5\n",
            "eval_iters = 40\n",
            "wandb_log = False # feel free to turn on\n",
            "wandb_project = 'shakespeare'\n",
            "wandb_run_name = 'ft-' + str(time.time())\n",
            "\n",
            "# only save checkpoints if the validation loss improves\n",
            "always_save_checkpoint = False\n",
            "\n",
            "# the number of examples per iter:\n",
            "# 1 batch_size * 32 grad_accum * 1024 tokens = 32,768 tokens/iter\n",
            "# shakespeare has 301,966 tokens, so 1 epoch ~= 9.2 iters\n",
            "batch_size = 1\n",
            "gradient_accumulation_steps = 32\n",
            "max_iters = 20\n",
            "\n",
            "# finetune at constant LR\n",
            "learning_rate = 2e-4\n",
            "decay_lr = False\n",
            "\n",
            "device = 'cuda'\n",
            "compile = False\n",
            "compute_grad_memory = True\n",
            "\n",
            "\n",
            "total number of tokens per iteration: 262144\n",
            "Initializing from OpenAI GPT-2 weights: gpt2\n",
            "2024-11-12 21:55:00.328595: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2024-11-12 21:55:00.345917: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-11-12 21:55:00.366965: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-11-12 21:55:00.373500: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-11-12 21:55:00.388753: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-11-12 21:55:01.573364: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "loading weights from pretrained gpt: gpt2\n",
            "forcing vocab_size=50257, block_size=1024, bias=True\n",
            "overriding dropout rate to 0.0\n",
            "overriding lora_rank and lora_alpha to 16\n",
            "overriding lora_dropout to 0\n",
            "number of parameters: 124.54M\n",
            "Marking model as LoRA fine-tunable...\n",
            "Done.\n",
            "/content/drive/MyDrive/nanoGPT-master/train.py:221: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))\n",
            "step 0: train loss 4.2459, val loss 4.0327\n",
            "grad memory usage: 3.38 MB\n",
            "iter 0: loss 4.4192, time 10845.91ms, mfu -100.00%\n",
            "grad memory usage: 3.38 MB\n",
            "iter 1: loss 3.5801, time 8711.69ms, mfu -100.00%\n",
            "grad memory usage: 3.38 MB\n",
            "iter 2: loss 3.5740, time 8626.97ms, mfu -100.00%\n",
            "grad memory usage: 3.38 MB\n",
            "iter 3: loss 3.6382, time 8642.56ms, mfu -100.00%\n",
            "grad memory usage: 3.38 MB\n",
            "iter 4: loss 3.8291, time 8595.99ms, mfu -100.00%\n",
            "step 5: train loss 3.7381, val loss 3.5825\n",
            "saving checkpoint to out-lora-shakespeare-gpt2-16-64\n",
            "grad memory usage: 3.38 MB\n",
            "iter 5: loss 3.8690, time 11096.00ms, mfu 6.52%\n",
            "grad memory usage: 3.38 MB\n",
            "iter 6: loss 3.9893, time 9166.45ms, mfu 6.65%\n",
            "grad memory usage: 3.38 MB\n",
            "iter 7: loss 4.4960, time 8522.22ms, mfu 6.84%\n",
            "grad memory usage: 3.38 MB\n",
            "iter 8: loss 3.7485, time 8612.93ms, mfu 6.99%\n",
            "grad memory usage: 3.38 MB\n",
            "iter 9: loss 3.8170, time 8649.67ms, mfu 7.13%\n",
            "step 10: train loss 3.7277, val loss 3.4851\n",
            "saving checkpoint to out-lora-shakespeare-gpt2-16-64\n",
            "grad memory usage: 3.38 MB\n",
            "iter 10: loss 4.0060, time 11244.04ms, mfu 7.06%\n",
            "grad memory usage: 3.38 MB\n",
            "iter 11: loss 3.6964, time 9186.01ms, mfu 7.14%\n",
            "grad memory usage: 3.38 MB\n",
            "iter 12: loss 3.8389, time 8648.83ms, mfu 7.26%\n",
            "grad memory usage: 3.38 MB\n",
            "iter 13: loss 3.0963, time 8683.45ms, mfu 7.37%\n",
            "grad memory usage: 3.38 MB\n",
            "iter 14: loss 3.7788, time 8582.77ms, mfu 7.47%\n",
            "step 15: train loss 3.6235, val loss 3.4555\n",
            "saving checkpoint to out-lora-shakespeare-gpt2-16-64\n",
            "grad memory usage: 3.38 MB\n",
            "iter 15: loss 3.9814, time 11232.60ms, mfu 7.37%\n",
            "grad memory usage: 3.38 MB\n",
            "iter 16: loss 3.8884, time 9032.98ms, mfu 7.43%\n",
            "grad memory usage: 3.38 MB\n",
            "iter 17: loss 3.9908, time 8566.96ms, mfu 7.53%\n",
            "grad memory usage: 3.38 MB\n",
            "iter 18: loss 4.0517, time 8576.54ms, mfu 7.62%\n",
            "grad memory usage: 3.38 MB\n",
            "iter 19: loss 3.6280, time 8490.61ms, mfu 7.71%\n",
            "step 20: train loss 3.6252, val loss 3.4424\n",
            "saving checkpoint to out-lora-shakespeare-gpt2-16-64\n",
            "grad memory usage: 3.38 MB\n",
            "iter 20: loss 3.7074, time 11210.21ms, mfu 7.59%\n"
          ]
        }
      ]
    }
  ]
}